{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### urllib3:\n",
    "\n",
    "- Permite fazer requisições e extrair os dados das páginas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "\n",
    "# PoolManager: Classe usada para fazer requisições http e pegar seu \n",
    "# conteúdo. Pode fazer um pool de conexões, ou seja, conexões com\n",
    "# várias páginas web.\n",
    "http = urllib3.PoolManager()\n",
    "\n",
    "# Requisição para uma pagina web\n",
    "pagina = http.request('GET', 'http://www.iaexpert.com.br')\n",
    "\n",
    "# Se o código retornado for 200 a conexão funcionou\n",
    "# Se o código retornado for 404 a conexão não funcionou\n",
    "pagina.status\n",
    "\n",
    "# Pegar todas informações da página (Código html)\n",
    "pagina.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extração de dados de HTML com Beautifullsoup\n",
    "\n",
    "- Permite a extração mais organizada dos dados da página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "\n",
    "# Disabilita warnings de certificado de verificação da requisição \n",
    "# urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "http = urllib3.PoolManager()\n",
    "pagina = http.request('GET', 'https://pt.wikipedia.org/wiki/Linguagem_de_programa%C3%A7%C3%A3o')\n",
    "pagina.status\n",
    "\n",
    "# soup = sopa (Representa todos os dados da página)\n",
    "soup = BeautifulSoup(pagina.data, 'lxml')\n",
    "# Retorna o(s) titulo(s) da página em formato string\n",
    "soup.title.string\n",
    "# Retorna todos os links da página\n",
    "links = soup.find_all('a')\n",
    "# Número de links encontrados\n",
    "len(links)\n",
    "# Mostra todos os links\n",
    "for link in links:\n",
    "    # Retorna o atributo o conteúdo do atributo recebido\n",
    "    print(link.get('href'))\n",
    "    # Retorna conteúdo (Título do link)\n",
    "    print(link.contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler - Busca de documentos 1\n",
    "\n",
    "- Percorre todos os links da página e adiciona todos os links em uma lista, depois abre todos os links e adiciona os seus links na lista, e repete o processo até uma determinada profundiade.\n",
    "- Baseado na página inicial, ele abre os links das páginas ligadas a ela.\n",
    "- As funções definidas abaixo são responsáveis pelo pré-processamento de dados, como indicado em sua descrição e como explicado mais abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "\n",
    "# Função que realiza o Pré-processamento dos textos\n",
    "# a partir da separação das palavras relevantes do texto e do armazenamento de seus radicais\n",
    "# as stopwords são desconsideradas\n",
    "# é permitido o armazenamento de palavras duplicadas\n",
    "def separaPalavras(texto):\n",
    "    stop = nltk.corpus.stopwords.words('portuguese')\n",
    "    splitter = re.compile('\\\\W+')\n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "    lista_palavras = []\n",
    "    lista = [p for p in splitter.split(texto) if p != '']\n",
    "\n",
    "    for p in lista:\n",
    "        if p.lower() not in stop:\n",
    "            if len(p) > 1:\n",
    "                lista_palavras.append(stemmer.stem(p).lower())\n",
    "\n",
    "    return lista_palavras\n",
    "\n",
    "\n",
    "# Função que realiza o Pré-processamento dos textos \n",
    "# a partir de remoção de tags HTML\n",
    "def getTexto(soup):\n",
    "    for tags in soup(['script', 'style']):\n",
    "        tags.decompose()\n",
    "\n",
    "    return ' '.join(soup.stripped_strings)\n",
    "\n",
    "\n",
    "# Importante: O urllib3 precisa da url completa, senão não funciona.\n",
    "\n",
    "# - Busca links e trata links das páginas recebida.\n",
    "# - A profundide indica o quão profundo será a busca, como ilustrado no Exemplo 1. \n",
    "# - Para uma busca muito profunda é necessário ter um computador potente e uma \n",
    "# grande banda de internet.\n",
    "\n",
    "# Exemplo 1:\n",
    "# profundidade = 1 -> Será percorrido e gerado os links apenas as lista de páginas recebidas.\n",
    "# profundidade = 2 -> Será percorrido e gerado os links, tanto da lista de páginas recebidas,\n",
    "# como da lista de novas páginas geradas a partir dos links gerados a partir da lista de páginas\n",
    "# recebidas com parâmetro.\n",
    "\n",
    "# Aumentar a profundidade significa repetir esse raciocíonio de maneira sucessiva.\n",
    "# Se for digitado uma profundidade muito grande será gerado um looping infinito.\n",
    "\n",
    "# Como esse é um código de teste, ele vai ser executado e não vai retorna nada.\n",
    "def crawl(paginas, profundidade):\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    for i in range(profundidade):\n",
    "        # É utilizado um set para evitar links repetidos\n",
    "        novas_paginas = set()\n",
    "        # Percorre todas as páginas passadas como parâmetro\n",
    "        for pagina in paginas:\n",
    "            http = urllib3.PoolManager()\n",
    "            try:\n",
    "                dados_pagina = http.request('GET', pagina)\n",
    "            except:\n",
    "                print(\"Erro ao abria página: \" + pagina)\n",
    "                continue\n",
    "            \n",
    "            soup = BeautifulSoup(dados_pagina.data, 'lxml')\n",
    "            links = soup.find_all('a')\n",
    "            # Contar quantos links válidos existem na página\n",
    "            # Links válidos: links que possuem um 'href'\n",
    "            contador = 0\n",
    "\n",
    "            for link in links:\n",
    "            # print(str(link.contents) + ' - ' + str(link.get('href')))\n",
    "\n",
    "            # Mostra todos atributos de um link\n",
    "            # print(link.attrs)\n",
    "            # print('\\n')\n",
    "\n",
    "            # Conta links válidos\n",
    "                if 'href' in link.attrs:\n",
    "                    # Junta uma url base com uma url relativa\n",
    "                    # Torna os links válidos, pois adiciona a parte base do\n",
    "                    # inicio que estava faltando.\n",
    "                    url = urljoin(pagina, str(link.get('href')))\n",
    "                    # if url != link.get('href'):\n",
    "                    #     print(url)\n",
    "                    #     print(link.get('href'))\n",
    "\n",
    "                    # Retira links inválidos\n",
    "                    if url.find(\"'\") != -1:\n",
    "                        continue\n",
    "                    \n",
    "                    # print(url)\n",
    "                    # Retira links que apontam para própria página\n",
    "                    url = url.split('#')[0]\n",
    "                    print(url)\n",
    "\n",
    "                    # Faz mais uma verificação de controle e\n",
    "                    # Armazena todos os links de cada página rodada no for\n",
    "                    if url[0:4] == 'http':\n",
    "                        novas_paginas.add(url)\n",
    "\n",
    "                    contador += 1\n",
    "\n",
    "            # Faz paginas receber novas páginas, depois que todos links\n",
    "            # de todas as páginas que foram recebidas como parâmetro foram \n",
    "            # armazenados no conjunto em novas páginas.\n",
    "            paginas = novas_paginas\n",
    "            print(contador)\n",
    "        print(len(novas_paginas))\n",
    "\n",
    "\n",
    "listapaginas = ['https://pt.wikipedia.org/wiki/Linguagem_de_programa%C3%A7%C3%A3o']\n",
    "crawl(listapaginas, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento dos textos - Remoção de tags HTML\n",
    "\n",
    "- Redução do conteúdo desnecessário que vem junto com o texto extraido na requsição.\n",
    "- Remoção das tags de 'style' e 'script'.\n",
    "- Pegar conteúdo das tags que sobraram e forma uma string gigante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "pagina = http.request('GET', 'https://pt.wikipedia.org/wiki/Linguagem_de_programa%C3%A7%C3%A3o')\n",
    "\n",
    "soup = BeautifulSoup(pagina.data, 'lxml')\n",
    "\n",
    "# Remove tags de script e style\n",
    "for tags in soup(['script', 'style']):\n",
    "    tags.decompose()\n",
    "\n",
    "# Junta todas as strings de conteúdo em uma só, e remove os espaços entre elas\n",
    "conteudo = ' '.join(soup.stripped_strings)\n",
    "\n",
    "# print(conteudo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento dos textos - Separação das Palavras\n",
    "\n",
    "- Recebendo uma string grande e quebrando em palavras, descosiderando as stops words.\n",
    "- Armazenar palavras únicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando expressões regulares. (Aplicar filtros em strings).\n",
    "import re \n",
    "# Importando biblioteca específica para processamento de linguagem natural.\n",
    "# -> Importante para identificação das stops words.\n",
    "# -> Stops words: Para que não tem significados sozinhas, portanto não são relevantes para identidade da página.\n",
    "import nltk \n",
    "\n",
    "# Fazer downloads do pacote da biblioteca \n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# - Forma uma lista de stop words de acordo com o idioma.\n",
    "# - Todas letras são em minúsculo, então é importante definir um padrão de letras minúsculas na verificação.\n",
    "stop = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "# - 1) Separação das palavras e Remoção das stops words:\n",
    "\n",
    "# W -> buscar qualquer caracter que não é um palavra.\n",
    "# + -> pode ter qualquer elemento .\n",
    "splitter = re.compile('\\\\W+')\n",
    "\n",
    "lista_palavras = []\n",
    "# - Quebra as palavras do texto dentro de split(), de maneira que quando encontra algum texto que não é um caracter\n",
    "# ele quebra a string passada como parâmetro e armazena na lista.\n",
    "# - Ignora os espaços vazios.\n",
    "\n",
    "# - Não funciona em todos os casos.\n",
    "# - Para funcionar para todos os casos é necessário adicionar mais filtros para tornar a indetificação de palavras válidas\n",
    "# mais eficiente.\n",
    "lista = [p for p in splitter.split('Este lugar lugar é apavorante a b c++') if p != '']\n",
    "\n",
    "for p in lista:\n",
    "    # Não adiciona as stops words ou letras sozinhas.\n",
    "    if p.lower() not in stop:\n",
    "        if len(p) > 1:\n",
    "            lista_palavras.append(p.lower())\n",
    "\n",
    "print(lista_palavras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento dos textos - Extração do radical\n",
    "\n",
    "- Extrair o radical de cada palavra que não é uma stopword.\n",
    "- Importante para diminuir o número de palavras que serão armazenadas na base de dados, pois como palavras que possuem o mesmo racial possuem o mesmo sentido, é possível fazer essa simplificação.\n",
    "- Segunda a maneira que foi implementado é possível adicionar palavras repetidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import nltk\n",
    "\n",
    "stop = nltk.corpus.stopwords.words('portuguese')\n",
    "# nltk.download('rslp')\n",
    "\n",
    "splitter = re.compile('\\\\W+')\n",
    "# RSLPStemmer: Classe usada para extrair o radical de cada palavra.\n",
    "# - Muito util para diminuir o número de palavras únicas a serem armazenadas no banco de dados\n",
    "# - Exemplo de extração de radical: \"nova\" e \"novamente\" podem ser armazenados apenas como uma palavra única, apartir de seu\n",
    "# radical \"nov\".\n",
    "\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "lista_palavras = []\n",
    "lista = [p for p in splitter.split('Este lugar lugar é apavorante a b c++') if p != '']\n",
    "\n",
    "for p in lista:\n",
    "    if p.lower() not in stop:\n",
    "        if len(p) > 1:\n",
    "            # Para uma codificação correta a alteração deve ser feita apenas na hora de armazenar a palavra na lista, para\n",
    "            # não ocorrer conflito na verificação das stopswords.\n",
    "            # Lembrando que: Nesse caso há a possibilidade de ser inseridos valores repetidos, então em vez de uma lista,\n",
    "            # pode ser mais interessante o uso de um set().\n",
    "            lista_palavras.append(stemmer.stem(p).lower())\n",
    "\n",
    "# stemmer.stem('nova')\n",
    "# stemmer.stem('novamente')\n",
    "\n",
    "print(lista_palavras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
