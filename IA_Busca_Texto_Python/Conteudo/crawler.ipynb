{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### urllib3:\n",
    "\n",
    "- Permite fazer requisições e extrair os dados das páginas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "\n",
    "# PoolManager: Classe usada para fazer requisições http e pegar seu \n",
    "# conteúdo. Pode fazer um pool de conexões, ou seja, conexões com\n",
    "# várias páginas web.\n",
    "http = urllib3.PoolManager()\n",
    "\n",
    "# Requisição para uma pagina web\n",
    "pagina = http.request('GET', 'http://www.iaexpert.com.br')\n",
    "\n",
    "# Se o código retornado for 200 a conexão funcionou\n",
    "# Se o código retornado for 404 a conexão não funcionou\n",
    "pagina.status\n",
    "\n",
    "# Pegar todas informações da página (Código html)\n",
    "pagina.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extração de dados de HTML com Beautifullsoup\n",
    "\n",
    "- Permite a extração mais organizada dos dados da página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "\n",
    "# Disabilita warnings de certificado de verificação da requisição \n",
    "# urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "http = urllib3.PoolManager()\n",
    "pagina = http.request('GET', 'https://pt.wikipedia.org/wiki/Linguagem_de_programa%C3%A7%C3%A3o')\n",
    "pagina.status\n",
    "\n",
    "# soup = sopa (Representa todos os dados da página)\n",
    "soup = BeautifulSoup(pagina.data, 'lxml')\n",
    "# Retorna o(s) titulo(s) da página em formato string\n",
    "soup.title.string\n",
    "# Retorna todos os links da página\n",
    "links = soup.find_all('a')\n",
    "# Número de links encontrados\n",
    "len(links)\n",
    "# Mostra todos os links\n",
    "for link in links:\n",
    "    # Retorna o atributo o conteúdo do atributo recebido\n",
    "    print(link.get('href'))\n",
    "    # Retorna conteúdo (Título do link)\n",
    "    print(link.contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler - Busca de documentos 1\n",
    "\n",
    "- Percorre todos os links da página e adiciona todos os links em uma lista, depois abre todos os links.\n",
    "- Baseado na página inicial, ele abre os links das páginas ligadas a ela.\n",
    "- Faz isso para todas as páginas até que não tenha mais páginas relacionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Importante: O urllib3 precisa da url completa, senão não funciona.\n",
    "\n",
    "# - Busca links e trata links das páginas recebida.\n",
    "# - A profundide indica o quão profundo será a busca, como ilustrado no Exemplo 1. \n",
    "# - Para uma busca muito profunda é necessário ter um computador potente e uma \n",
    "# grande banda de internet.\n",
    "\n",
    "# Exemplo 1:\n",
    "# profundidade = 1 -> Será percorrido e gerado os links apenas as lista de páginas recebidas.\n",
    "# profundidade = 2 -> Será percorrido e gerado os links, tanto da lista de páginas recebidas,\n",
    "# como da lista de novas páginas geradas a partir dos links gerados a partir da lista de páginas\n",
    "# recebidas com parâmetro.\n",
    "\n",
    "# Aumentar a profundidade significa repetir esse raciocíonio de maneira sucessiva.\n",
    "# Se for digitado uma profundidade muito grande será gerado um looping infinito.\n",
    "\n",
    "# Como esse é um código de teste, ele vai ser executado e não vai retorna nada.\n",
    "def crawl(paginas, profundidade):\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    for i in range(profundidade):\n",
    "        # É utilizado um set para evitar links repetidos\n",
    "        novas_paginas = set()\n",
    "        # Percorre todas as páginas passadas como parâmetro\n",
    "        for pagina in paginas:\n",
    "            http = urllib3.PoolManager()\n",
    "            try:\n",
    "                dados_pagina = http.request('GET', pagina)\n",
    "            except:\n",
    "                print(\"Erro ao abria página: \" + pagina)\n",
    "                continue\n",
    "            \n",
    "            soup = BeautifulSoup(dados_pagina.data, 'lxml')\n",
    "            links = soup.find_all('a')\n",
    "            # Contar quantos links válidos existem na página\n",
    "            # Links válidos: links que possuem um 'href'\n",
    "            contador = 0\n",
    "\n",
    "            for link in links:\n",
    "            # print(str(link.contents) + ' - ' + str(link.get('href')))\n",
    "\n",
    "            # Mostra todos atributos de um link\n",
    "            # print(link.attrs)\n",
    "            # print('\\n')\n",
    "\n",
    "            # Conta links válidos\n",
    "                if 'href' in link.attrs:\n",
    "                    # Junta uma url base com uma url relativa\n",
    "                    # Torna os links válidos, pois adiciona a parte base do\n",
    "                    # inicio que estava faltando.\n",
    "                    url = urljoin(pagina, str(link.get('href')))\n",
    "                    # if url != link.get('href'):\n",
    "                    #     print(url)\n",
    "                    #     print(link.get('href'))\n",
    "\n",
    "                    # Retira links inválidos\n",
    "                    if url.find(\"'\") != -1:\n",
    "                        continue\n",
    "                    \n",
    "                    # print(url)\n",
    "                    # Retira links que apontam para própria página\n",
    "                    url = url.split('#')[0]\n",
    "                    print(url)\n",
    "\n",
    "                    # Faz mais uma verificação de controle e\n",
    "                    # Armazena todos os links de cada página rodada no for\n",
    "                    if url[0:4] == 'http':\n",
    "                        novas_paginas.add(url)\n",
    "\n",
    "                    contador += 1\n",
    "\n",
    "            # Faz paginas receber novas páginas, depois que todos links\n",
    "            # de todas as páginas que foram recebidas como parâmetro foram \n",
    "            # armazenados no conjunto em novas páginas.\n",
    "            paginas = novas_paginas\n",
    "            print(contador)\n",
    "        print(len(novas_paginas))\n",
    "\n",
    "\n",
    "listapaginas = ['https://pt.wikipedia.org/wiki/Linguagem_de_programa%C3%A7%C3%A3o']\n",
    "crawl(listapaginas, 1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
