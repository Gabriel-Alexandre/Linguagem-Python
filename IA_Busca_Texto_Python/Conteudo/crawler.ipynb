{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### urllib3:\n",
    "\n",
    "- Permite fazer requisições e extrair os dados das páginas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "\n",
    "# PoolManager: Classe usada para fazer requisições http e pegar seu \n",
    "# conteúdo. Pode fazer um pool de conexões, ou seja, conexões com\n",
    "# várias páginas web.\n",
    "http = urllib3.PoolManager()\n",
    "\n",
    "# Requisição para uma pagina web\n",
    "pagina = http.request('GET', 'http://www.iaexpert.com.br')\n",
    "\n",
    "# Se o código retornado for 200 a conexão funcionou\n",
    "# Se o código retornado for 404 a conexão não funcionou\n",
    "pagina.status\n",
    "\n",
    "# Pegar todas informações da página (Código html)\n",
    "pagina.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extração de dados de HTML com Beautifullsoup\n",
    "\n",
    "- Permite a extração mais organizada dos dados da página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "\n",
    "# Disabilita warnings de certificado de verificação da requisição \n",
    "# urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "http = urllib3.PoolManager()\n",
    "pagina = http.request('GET', 'https://pt.wikipedia.org/wiki/Linguagem_de_programa%C3%A7%C3%A3o')\n",
    "pagina.status\n",
    "\n",
    "# soup = sopa (Representa todos os dados da página)\n",
    "soup = BeautifulSoup(pagina.data, 'lxml')\n",
    "# Retorna o(s) titulo(s) da página em formato string\n",
    "soup.title.string\n",
    "# Retorna todos os links da página\n",
    "links = soup.find_all('a')\n",
    "# Número de links encontrados\n",
    "len(links)\n",
    "# Mostra todos os links\n",
    "for link in links:\n",
    "    # Retorna o atributo o conteúdo do atributo recebido\n",
    "    print(link.get('href'))\n",
    "    # Retorna conteúdo (Título do link)\n",
    "    print(link.contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler - Busca de documentos 1\n",
    "\n",
    "- Percorre todos os links da página e adiciona todos os links em uma lista, depois abre todos os links.\n",
    "- Baseado na página inicial, ele abre os links das páginas ligadas a ela.\n",
    "- Faz isso para todas as páginas até que não tenha mais páginas relacionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Importante: O urllib3 precisa da url completa, senão não funciona.\n",
    "\n",
    "\n",
    "def crawl(pagina):\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    http = urllib3.PoolManager()\n",
    "    try:\n",
    "        dados_pagina = http.request('GET', pagina)\n",
    "    except:\n",
    "        print(\"Erro ao abria página: \" + pagina)\n",
    "    \n",
    "    soup = BeautifulSoup(dados_pagina.data, 'lxml')\n",
    "    links = soup.find_all('a')\n",
    "    # Contar quantos links válidos existem na página\n",
    "    # Links válidos: links que possuem um 'href'\n",
    "    contador = 0\n",
    "\n",
    "    for link in links:\n",
    "        # print(str(link.contents) + ' - ' + str(link.get('href')))\n",
    "\n",
    "        # Mostra todos atributos de um link\n",
    "        # print(link.attrs)\n",
    "        # print('\\n')\n",
    "\n",
    "        # Conta links válidos\n",
    "        if 'href' in link.attrs:\n",
    "            # Junta uma url base com uma url relativa\n",
    "            # Torna os links válidos, pois adiciona a parte base do\n",
    "            # inicio que estava faltando.\n",
    "            url = urljoin(pagina, str(link.get('href')))\n",
    "            # if url != link.get('href'):\n",
    "            #     print(url)\n",
    "            #     print(link.get('href'))\n",
    "\n",
    "            # Retira links inválidos\n",
    "            if url.find(\"'\") != -1:\n",
    "                continue\n",
    "            \n",
    "            # print(url)\n",
    "            # Retira links que apontam para própria página\n",
    "            url = url.split('#')[0]\n",
    "            print(url)\n",
    "\n",
    "            contador += 1\n",
    "    print(contador)\n",
    "\n",
    "\n",
    "crawl('https://pt.wikipedia.org/wiki/Linguagem_de_programa%C3%A7%C3%A3o')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
