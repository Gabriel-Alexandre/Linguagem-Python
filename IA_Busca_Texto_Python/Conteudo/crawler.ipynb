{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### urllib3:\n",
    "\n",
    "- Permite fazer requisições e extrair os dados das páginas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "\n",
    "# PoolManager: Classe usada para fazer requisições http e pegar seu \n",
    "# conteúdo. Pode fazer um pool de conexões, ou seja, conexões com\n",
    "# várias páginas web.\n",
    "http = urllib3.PoolManager()\n",
    "\n",
    "# Requisição para uma pagina web\n",
    "pagina = http.request('GET', 'http://www.iaexpert.com.br')\n",
    "\n",
    "# Se o código retornado for 200 a conexão funcionou\n",
    "# Se o código retornado for 404 a conexão não funcionou\n",
    "pagina.status\n",
    "\n",
    "# Pegar todas informações da página (Código html)\n",
    "pagina.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extração de dados de HTML com Beautifullsoup\n",
    "\n",
    "- Permite a extração mais organizada dos dados da página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "\n",
    "# Disabilita warnings de certificado de verificação da requisição \n",
    "# urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "http = urllib3.PoolManager()\n",
    "pagina = http.request('GET', 'https://pt.wikipedia.org/wiki/Linguagem_de_programa%C3%A7%C3%A3o')\n",
    "pagina.status\n",
    "\n",
    "# soup = sopa (Representa todos os dados da página)\n",
    "soup = BeautifulSoup(pagina.data, 'lxml')\n",
    "# Retorna o(s) titulo(s) da página em formato string\n",
    "soup.title.string\n",
    "# Retorna todos os links da página\n",
    "links = soup.find_all('a')\n",
    "# Número de links encontrados\n",
    "len(links)\n",
    "# Mostra todos os links\n",
    "for link in links:\n",
    "    # Retorna o atributo o conteúdo do atributo recebido\n",
    "    print(link.get('href'))\n",
    "    # Retorna conteúdo (Título do link)\n",
    "    print(link.contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler - Busca de documentos 1\n",
    "\n",
    "- Percorre todos os links da página e adiciona todos os links em uma lista, depois abre todos os links e adiciona os seus links na lista, e repete o processo até uma determinada profundiade.\n",
    "- Baseado na página inicial, ele abre os links das páginas ligadas a ela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "import re\n",
    "import nltk\n",
    "# Biblioteca responsável por liga o python ao banco de dados mysql\n",
    "import pymysql\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "# (use_unicode e charset): Atributos que indicam a formatação a ser utilizada na base de dados.\n",
    "#   - Isso é importante para palavras retiradas da web, para evitar erros de formatação.\n",
    "\n",
    "# Função que indexa url recebida como parâmetro e suas palavras, na base dados.\n",
    "#   - Palavras são inseridas na tabela de palavras.\n",
    "#   - Url é inserida na tabela de urls.\n",
    "#   - É realizado a ligação de cada palavra a url, assim como é indicado a localização que a palavra se encontra no documento.\n",
    "def indexador(url, sopa):\n",
    "    # Verifica se url já foi indexada.\n",
    "    indexada = paginaIndexada(url)\n",
    "    # Se a url já foi indexada a função retorna.\n",
    "    if indexada == -2:\n",
    "        return\n",
    "    # Se a url ainda não foi indexada, ela é inserida na base de dados.\n",
    "    elif indexada == -1:\n",
    "        idnova_pagina = insertPagina(url)\n",
    "    # Se a url já foi indexada, porém ainda não tem palavras cadastradas, seu endereço é armazenado.\n",
    "    elif indexada > 0:\n",
    "        idnova_pagina = indexada\n",
    "\n",
    "    # Recebendo conteúdo principal de cada página como uma grande string.    \n",
    "    texto = getTexto(sopa)\n",
    "    # Separando palavras da página a partir de seus radicais e armazenando todos os radicais em uma lista.\n",
    "    palavras = separaPalavras(texto)\n",
    "\n",
    "    for i in range(len(palavras)):\n",
    "        palavra = palavras[i]\n",
    "        # Verificando se a palavra já está indexada na base de dados.\n",
    "        # Se não estiver indexada na base de dados, inserir.\n",
    "        idpalavra = palavraIndexada(palavra)\n",
    "        if idpalavra == -1:\n",
    "            idpalavra = insertPalavra(palavra)\n",
    "        # Unindo palavra a url, na base dados.\n",
    "        # Informando a localização no qual a palavra se encontra na base de dados.\n",
    "        # Para essa função, todas as palavras vão ser inseridas.\n",
    "        insertPalavraLocalizacao(idnova_pagina, idpalavra, i)    \n",
    "\n",
    "\n",
    "def separaPalavras(texto):\n",
    "    stop = nltk.corpus.stopwords.words('portuguese')\n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "    splitter = re.compile('\\\\W+')\n",
    "    lista_palavras = []\n",
    "    lista = [p for p in splitter.split(texto) if p != '']\n",
    "    for p in lista:\n",
    "        if stemmer.stem(p.lower()) not in stop:\n",
    "            if len(p) > 1:\n",
    "                lista_palavras.append(stemmer.stem(p.lower()))\n",
    "    return lista_palavras\n",
    "\n",
    "\n",
    "# - Função que une uma palavra a uma url, inserindo essa informação na base de dados.\n",
    "# - Também é inserido a localização no documento dessa palavra na página.\n",
    "#   - Isso vai ser usado mais na frente para a otimização dos resultados de busca.\n",
    "def insertPalavraLocalizacao(idurl, idpalavra, localizacao):\n",
    "    conexao = pymysql.connect(host='localhost', user='root', passwd='123456', db='indice', autocommit = True)\n",
    "    cursor = conexao.cursor()\n",
    "    # Inserido \"idurl\", \"idpalavra\" e \"localizacao\" recebidos como parâmetro, na tabela de palavra_localizacao.\n",
    "    cursor.execute(\"insert into palavra_localizacao (idurl, idpalavra, localizacao) values (%s, %s, %s)\", (idurl, idpalavra, localizacao))\n",
    "    idpalavra_localizacao = cursor.lastrowid\n",
    "    cursor.close()\n",
    "    conexao.close()\n",
    "    # Retornar o id da palavra_localizacao.\n",
    "    return idpalavra_localizacao\n",
    "\n",
    "\n",
    "# - Função que insere a palavra recebida como parâmetro, na tabela de palavras.\n",
    "def insertPalavra(palavra):\n",
    "    conexao = pymysql.connect(host='localhost', user='root', passwd='123456', db='indice', autocommit = True, use_unicode=True, charset=\"utf8mb4\")\n",
    "    cursor = conexao.cursor()\n",
    "    # - Inserindo palavra recebida, na tabela de palavras da base de dados.\n",
    "    # - O id de cada palavra é gerado automaticamente conforme a inserção das palavras.\n",
    "    cursor.execute(\"insert into palavras (palavra) values (%s)\", palavra)\n",
    "    idpalavra = cursor.lastrowid\n",
    "    cursor.close()\n",
    "    conexao.close()\n",
    "    # Retornar o id no qual a palavra foi inserida.\n",
    "    return idpalavra\n",
    "\n",
    "\n",
    "# - Função que verifica se uma palavra já foi indexada na tabela de palavras da base de dados.\n",
    "# - Seu principal uso é para controlar as palavras que são inseridas na base de dados, de maneira que não seja possível inserir palavras repetidas.\n",
    "def palavraIndexada(palavra):\n",
    "    # (retorno == -1) -> palavra não cadastrada.\n",
    "    retorno = -1\n",
    "    conexao = pymysql.connect(host='localhost', user='root', passwd='123456', db='indice', use_unicode=True, charset=\"utf8mb4\")\n",
    "    cursor = conexao.cursor()\n",
    "    # Realizando consulta para verificar se existe alguma palavra igual a palavra recebida como parâmetro.\n",
    "    cursor.execute('select idpalavra from palavras where palavra = %s', palavra)\n",
    "    # Se algum registro foi retornado, quer dizer que a palavra já existe na base de dados.\n",
    "    if cursor.rowcount > 0:\n",
    "        # (retorno = cursor.fetchone()[0]) -> palavra cadastrada, então a função retorna o seu idpalavra.\n",
    "        retorno = cursor.fetchone()[0]\n",
    "    cursor.close()\n",
    "    conexao.close()\n",
    "        \n",
    "    return retorno\n",
    "\n",
    "\n",
    "# - Função que insere a url recebida como parâmetro, na tabela de urls.\n",
    "def insertPagina(url):\n",
    "    # (autocommit): Permite gravar alterações na base de dados.\n",
    "    conexao = pymysql.connect(host='localhost', user='root', passwd='123456', db='indice', autocommit = True, use_unicode=True, charset=\"utf8mb4\")\n",
    "    cursor = conexao.cursor()\n",
    "    # - Inserindo url recebida, na tabela de urls da base de dados.\n",
    "    # - O id de cada url é gerado automaticamente conforme a inserção das urls.\n",
    "    cursor.execute(\"insert into urls (url) values (%s)\", url)\n",
    "    # lastrowid: retorna o último id que foi inserido na base de dados.\n",
    "    # - Para execuções concorrentes/paralelas isso é um problema, porém como o objetivo desse crawler é o aprendizado então é válido.\n",
    "    idpagina = cursor.lastrowid\n",
    "    cursor.close()\n",
    "    conexao.close()\n",
    "    # Retornar o id no qual a url foi inserida.\n",
    "    return idpagina\n",
    "\n",
    "\n",
    "# - Função que verifica se a url de uma página já foi indexada na base de dados.\n",
    "def paginaIndexada(url):\n",
    "    # (retorno == -1) -> url não cadastrada\n",
    "    retorno = -1\n",
    "    # Criando conexão com a base de dados.\n",
    "    conexao = pymysql.connect(host='localhost', user='root', passwd='123456', db='indice')\n",
    "    # cursor(): objeto da biblioteca pymsql responsável pela realização das consultas ao banco de dados.\n",
    "    \n",
    "    # Criando cursor() para url.\n",
    "    cursorUrl = conexao.cursor()\n",
    "    # execute(): método de cursor() que possibilita a execução de consultas a base de dados.\n",
    "    \n",
    "    # Realizando consulta para verificar se existe alguma url igual a url passada cadastrada na base de dados.\n",
    "    cursorUrl.execute('select idurl from urls where url = %s', url)\n",
    "    # rowcount: retorna o número de linhas da tabela que foi consultada.\n",
    "    \n",
    "    # - Se o número de linhas for maior que zero, quer dizer a url recebida já foi cadastrada na base de dados, anteriormente.\n",
    "    if cursorUrl.rowcount > 0:\n",
    "        # - Se url já está cadastrada, verificar se já existe alguma palavra cadastrada para essa url.\n",
    "        \n",
    "        # Recebendo o idurl da url.\n",
    "        idurl = cursorUrl.fetchone()[0]\n",
    "        # Criando cursor() para palavra.\n",
    "        cursorPalavra = conexao.cursor()\n",
    "        # Realizando consulta para verificar se existe alguma palavra cadastrada na tabela palavra_localizacao da base de dados.\n",
    "        cursorPalavra.execute('select idurl from palavra_localizacao where idurl = %s', idurl)\n",
    "        # - Se o número de linhas for maior que zero, quer dizer que existe palavras cadastrada para a url na base de dados.\n",
    "        if cursorPalavra.rowcount > 0:\n",
    "            # (retorno == -2) -> url cadastrada com palavras cadastradas.\n",
    "            retorno = -2\n",
    "        else:\n",
    "            # (retorno == idurl) -> url cadastrada sem palavras cadastradas.\n",
    "            retorno = idurl\n",
    "        # Fechamento do cursor. (Liberando memória, pois isso é realizado dinâmicamente).\n",
    "        cursorPalavra.close()\n",
    "    # Fechamento do cursor. (Liberando memória, pois isso é realizado dinâmicamente).\n",
    "    cursorUrl.close()\n",
    "    # Fechamento da conexão com a base de dados. (Liberando memória, pois isso é realizado dinâmicamente).\n",
    "    conexao.close()\n",
    "    \n",
    "    return retorno\n",
    "\n",
    "\n",
    "# - Função que realiza o Pré-processamento dos textos a partir da separação das palavras relevantes do texto e do armazenamento \n",
    "# de seus radicais.\n",
    "# - As stopwords são desconsideradas.\n",
    "# - É permitido o armazenamento de palavras duplicadas.\n",
    "# - Retorna uma lista de palavras, considerando os parâmetro acima.\n",
    "def separaPalavras(texto):\n",
    "    stop = nltk.corpus.stopwords.words('portuguese')\n",
    "    splitter = re.compile('\\\\W+')\n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "    lista_palavras = []\n",
    "    lista = [p for p in splitter.split(texto) if p != '']\n",
    "\n",
    "    for p in lista:\n",
    "        if p.lower() not in stop:\n",
    "            if len(p) > 1:\n",
    "                lista_palavras.append(stemmer.stem(p).lower())\n",
    "    return lista_palavras\n",
    "\n",
    "\n",
    "# - Função que realiza o Pré-processamento dos textos a partir de remoção de tags HTML.\n",
    "# - Retorna uma string gigante que contém o conteúdo relevante de cada página.\n",
    "def getTexto(soup):\n",
    "    for tags in soup(['script', 'style']):\n",
    "        tags.decompose()\n",
    "\n",
    "    return ' '.join(soup.stripped_strings)\n",
    "\n",
    "\n",
    "# Importante: O urllib3 precisa da url completa, senão não funciona.\n",
    "\n",
    "# - Busca links e trata links das páginas recebida.\n",
    "# - A profundide indica o quão profundo será a busca, como ilustrado no Exemplo 1. \n",
    "# - Para uma busca muito profunda é necessário ter um computador potente e uma \n",
    "# grande banda de internet.\n",
    "\n",
    "# Exemplo 1:\n",
    "# profundidade = 1 -> Será percorrido e gerado os links apenas da lista de páginas recebidas.\n",
    "# profundidade = 2 -> Será percorrido e gerado os links, tanto da lista de páginas recebidas,\n",
    "# como da lista de novas páginas geradas a partir dos links gerados a partir da lista de páginas\n",
    "# recebidas com parâmetro.\n",
    "\n",
    "# - Aumentar a profundidade significa repetir esse raciocíonio de maneira sucessiva.\n",
    "# - Se for digitado uma profundidade muito grande será gerado um \"looping infinito\".\n",
    "\n",
    "# Como esse é um código de teste, ele vai ser executado e não vai retorna nada.\n",
    "def crawl(paginas, profundidade):\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "    for i in range(profundidade):\n",
    "        # É utilizado um set para evitar links repetidos\n",
    "        novas_paginas = set()\n",
    "        # Percorre todas as páginas passadas como parâmetro\n",
    "        for pagina in paginas:\n",
    "            http = urllib3.PoolManager()\n",
    "            try:\n",
    "                dados_pagina = http.request('GET', pagina)\n",
    "            except:\n",
    "                print(\"Erro ao abria página: \" + pagina)\n",
    "                continue\n",
    "            \n",
    "            soup = BeautifulSoup(dados_pagina.data, 'lxml')\n",
    "            # Indexando página\n",
    "            indexador(pagina, soup)\n",
    "            links = soup.find_all('a')\n",
    "            \n",
    "            # Percorrer links da página e adicionalos a estrutura de \"novas_paginas\"\n",
    "            for link in links:\n",
    "                # Se for um link válido, ele entra nessa condição\n",
    "                if 'href' in link.attrs:\n",
    "                    # - Junta uma url base com uma url relativa\n",
    "                    # - Torna os links válidos, pois adiciona a parte base do inicio que estava faltando.\n",
    "                    url = urljoin(pagina, str(link.get('href')))\n",
    "                    # Retira links inválidos\n",
    "                    if url.find(\"'\") != -1:\n",
    "                        continue\n",
    "\n",
    "                    # Retira links que apontam para própria página\n",
    "                    url = url.split('#')[0]\n",
    "\n",
    "                    # Faz mais uma verificação de controle e armazena todos os links de cada página rodada na estrutua \"novas_paginas\".\n",
    "                    if url[0:4] == 'http':\n",
    "                        novas_paginas.add(url)\n",
    "\n",
    "            # - Faz paginas receber novas páginas, depois que todos links de todas as páginas que foram recebidas como parâmetro foram \n",
    "            # armazenados no conjunto \"novas páginas\".\n",
    "            # - Isso vale tanto pra primeira execução, quanto para execução em uma profundidade maior.\n",
    "            paginas = novas_paginas\n",
    "\n",
    "\n",
    "# listapaginas = ['https://pt.wikipedia.org/wiki/Linguagem_de_programa%C3%A7%C3%A3o']\n",
    "# listapaginas = ['https://dhauz.com/']\n",
    "# crawl(listapaginas, 1)\n",
    "\n",
    "# Próximos passos:\n",
    "\n",
    "# 1 - Deletar base de dados (OK)\n",
    "# 2 - Terminar de assistir aulas do módulo (OK)\n",
    "# 3 - Organizar anotações e simplificar todas anotações do módulo. (Revisão). (OK)\n",
    "# 4 - Começar próximo módulo e seguir metodologia semelhante.\n",
    "\n",
    "# Sugestões:\n",
    "\n",
    "# 1- Quando terminar o curso, otimizar pontos de melhoria do código. (Sempre testando para ver se está tudo ok)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento dos textos - Remoção de tags HTML\n",
    "\n",
    "- Redução do conteúdo desnecessário que vem junto com o texto extraido na requsição.\n",
    "- Remoção das tags de 'style' e 'script'.\n",
    "- Pegar conteúdo das tags que sobraram e forma uma string gigante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "pagina = http.request('GET', 'https://pt.wikipedia.org/wiki/Linguagem_de_programa%C3%A7%C3%A3o')\n",
    "\n",
    "soup = BeautifulSoup(pagina.data, 'lxml')\n",
    "\n",
    "# Remove tags de script e style\n",
    "for tags in soup(['script', 'style']):\n",
    "    tags.decompose()\n",
    "\n",
    "# Junta todas as strings de conteúdo em uma só, e remove os espaços entre elas\n",
    "conteudo = ' '.join(soup.stripped_strings)\n",
    "\n",
    "# print(conteudo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento dos textos - Separação das Palavras\n",
    "\n",
    "- Recebendo uma string grande e quebrando em palavras, descosiderando as stops words.\n",
    "- Armazenar palavras únicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando expressões regulares. (Aplicar filtros em strings).\n",
    "import re \n",
    "# Importando biblioteca específica para processamento de linguagem natural.\n",
    "# -> Importante para identificação das stops words.\n",
    "# -> Stops words: Para que não tem significados sozinhas, portanto não são relevantes para identidade da página.\n",
    "import nltk \n",
    "\n",
    "# Fazer downloads do pacote da biblioteca \n",
    "# nltk.download('stopwords')\n",
    "\n",
    "# - Forma uma lista de stop words de acordo com o idioma.\n",
    "# - Todas letras são em minúsculo, então é importante definir um padrão de letras minúsculas na verificação.\n",
    "stop = nltk.corpus.stopwords.words('portuguese')\n",
    "\n",
    "# - 1) Separação das palavras e Remoção das stops words:\n",
    "\n",
    "# W -> buscar qualquer caracter que não é um palavra.\n",
    "# + -> pode ter qualquer elemento .\n",
    "splitter = re.compile('\\\\W+')\n",
    "\n",
    "lista_palavras = []\n",
    "# - Quebra as palavras do texto dentro de split(), de maneira que quando encontra algum texto que não é um caracter\n",
    "# ele quebra a string passada como parâmetro e armazena na lista.\n",
    "# - Ignora os espaços vazios.\n",
    "\n",
    "# - Não funciona em todos os casos.\n",
    "# - Para funcionar para todos os casos é necessário adicionar mais filtros para tornar a indetificação de palavras válidas\n",
    "# mais eficiente.\n",
    "lista = [p for p in splitter.split('Este lugar lugar é apavorante a b c++') if p != '']\n",
    "\n",
    "for p in lista:\n",
    "    # Não adiciona as stops words ou letras sozinhas.\n",
    "    if p.lower() not in stop:\n",
    "        if len(p) > 1:\n",
    "            lista_palavras.append(p.lower())\n",
    "\n",
    "print(lista_palavras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento dos textos - Extração do radical\n",
    "\n",
    "- Extrair o radical de cada palavra que não é uma stopword.\n",
    "- Importante para diminuir o número de palavras que serão armazenadas na base de dados, pois como palavras que possuem o mesmo racial possuem o mesmo sentido é possível fazer essa simplificação.\n",
    "- Segundo a maneira que foi implementado é possível adicionar palavras repetidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import nltk\n",
    "\n",
    "stop = nltk.corpus.stopwords.words('portuguese')\n",
    "# nltk.download('rslp')\n",
    "\n",
    "splitter = re.compile('\\\\W+')\n",
    "# RSLPStemmer: Classe usada para extrair o radical de cada palavra.\n",
    "# - Muito util para diminuir o número de palavras únicas a serem armazenadas no banco de dados\n",
    "# - Exemplo de extração de radical: \"nova\" e \"novamente\" podem ser armazenados apenas como uma palavra única, apartir de seu\n",
    "# radical \"nov\".\n",
    "\n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "\n",
    "lista_palavras = []\n",
    "lista = [p for p in splitter.split('Este lugar lugar é apavorante a b c++') if p != '']\n",
    "\n",
    "for p in lista:\n",
    "    if p.lower() not in stop:\n",
    "        if len(p) > 1:\n",
    "            # Para uma codificação correta a alteração deve ser feita apenas na hora de armazenar a palavra na lista, para\n",
    "            # não ocorrer conflito na verificação das stopswords.\n",
    "            # Lembrando que: Nesse caso há a possibilidade de ser inseridos valores repetidos, então em vez de uma lista,\n",
    "            # pode ser mais interessante o uso de um set().\n",
    "            lista_palavras.append(stemmer.stem(p).lower())\n",
    "\n",
    "# stemmer.stem('nova')\n",
    "# stemmer.stem('novamente')\n",
    "\n",
    "print(lista_palavras)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
